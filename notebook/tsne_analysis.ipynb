{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493e001-39ba-45e3-aeb1-2ed1db7682de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "sys.path.append('../')\n",
    "from dataset.hf_imagenet_dataset import HFImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f394e-7379-4af5-8e43-71fd7e6db2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dir = '../weights_CL_imagenet/with_inst_params_lr_0.8_wd_1e-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3a1d6-3e59-4b59-a77a-a828cf4e3722",
   "metadata": {},
   "source": [
    "## Load  instance-level parameters from all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6fcfc-3b3f-4b5e-bbd6-0dfde2186901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the loaded checkpoints\n",
    "epoch_instance_level_temp = [] # (nr_epochs, nr_instances)\n",
    "total_epochs = 120\n",
    "\n",
    "# Loop through the checkpoint files and load each one\n",
    "for epoch in tqdm(range(total_epochs), desc='Loading instance level parameters from all checkpoints'):\n",
    "    checkpoint_path = f'{analysis_dir}/epoch_{epoch}.pth.tar'\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        epoch_instance_level_temp.append(checkpoint['inst_parameters'])\n",
    "    else:\n",
    "        print(f\"Checkpoint file {checkpoint_path} not found.\")\n",
    "\n",
    "# Convert the entries to temperature and vertically stack across epochs\n",
    "epoch_instance_level_temp = np.exp(np.vstack(epoch_instance_level_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4e03f-1fc0-416a-8e8a-351a346ccb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_convergence = epoch_instance_level_temp[-1,:]\n",
    "print(f'Mean temperature at convergence: {np.mean(temp_convergence) :0.1f}')\n",
    "print(f'Max temperature at convergence: {np.max(temp_convergence) :0.1f}')\n",
    "print(f'Min temperature at convergence: {np.min(temp_convergence) :0.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd87dc-f0a2-4f97-be6e-33a62186526d",
   "metadata": {},
   "source": [
    "## Load labels for entire dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b995440-af69-4268-b4f3-d47bc27b343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_dataset = HFImageNet(split='train', transform=None)\n",
    "label_instances = np.array(imagenet_dataset.dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f12c6c-8f24-48e8-9090-26ba52bf15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of label-ids to name of classes\n",
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"imagenet-1k-id2label.json\"\n",
    "id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n",
    "id2label = {int(k):v for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc08506-2332-42b7-968c-2ca339f92944",
   "metadata": {},
   "source": [
    "## Analyze samples of particular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7a696-199a-4af0-a3dd-65e4a78c2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx = 145\n",
    "\n",
    "# Find instances of this class and their temperature value\n",
    "instance_class_idx = np.where(label_instances == class_idx)[0] # (N, 1)\n",
    "instance_class_temp_convergence = temp_convergence[instance_class_idx] # (N, 1)\n",
    "instance_class_temp_all_epochs = epoch_instance_level_temp[:, instance_class_idx] # (nr_epochs, N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb53ee6-c367-4c62-9187-b363ba0d258f",
   "metadata": {},
   "source": [
    "## Compute T-SNE embedding of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0a671b-e031-44c9-a794-2cd88e6cba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_instances = instance_class_temp_all_epochs.T #(N, D=nr_epochs)\n",
    "coordinates = TSNE(n_components=2, learning_rate='auto', init='pca', perplexity=50).fit_transform(features_instances) # (N, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2115ee-46d1-42e6-bdd6-84a715545379",
   "metadata": {},
   "source": [
    "## Plot the result of T-SNE in a chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099714f8-cd63-46a5-97d4-4b8d176297de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the list of images for all instances in this class\n",
    "list_all_instances_images = [imagenet_dataset.dataset[int(idx)]['image'] for idx in instance_class_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf96ba77-bf93-4724-842b-f6b124e7f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(instance_class_temp_all_epochs[-1, :] == instance_class_temp_convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891b28f-288a-4fa9-a466-a6eeb5c48ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify each instance as easy vs hard based on temperature value and use it to color the border\n",
    "all_border_label = []\n",
    "for idx in range(len(instance_class_idx)):\n",
    "    if instance_class_temp_convergence[idx] > 1.6:\n",
    "        all_border_label.append('red')\n",
    "    else:\n",
    "        all_border_label.append('green')\n",
    "\n",
    "    # Check if this is an image which was ignored at the start but then learnt towards the end\n",
    "    temperature_trajectory = instance_class_temp_all_epochs[:, idx]\n",
    "    delta_high_low = np.max(temperature_trajectory) - np.min(temperature_trajectory)\n",
    "\n",
    "    if np.max(temperature_trajectory) > 1.6 and delta_high_low > 0.5 and temperature_trajectory[-1] < 1.0:\n",
    "        all_border_label[-1] = 'orange'\n",
    "    \n",
    "outliers = (np.array(all_border_label) == 'red').sum()\n",
    "active_learning = (np.array(all_border_label) == 'orange').sum()\n",
    "print(f'{outliers} elements classifed as outliers')\n",
    "print(f'{active_learning} elements classifed as active-learning candidates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194266e-f2c3-4ae0-9562-556c8efc7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot each image\n",
    "def plot_image(x, y, image, ax, border_color='white'):\n",
    "    # Add a white border to the image\n",
    "    border_size = 10  # Adjust border size as needed\n",
    "    image = ImageOps.expand(image, border=border_size, fill=border_color)\n",
    "    \n",
    "    im = OffsetImage(image, zoom=0.2)  # Adjust zoom as needed\n",
    "    ab = AnnotationBbox(im, (x, y), frameon=False, pad=0)\n",
    "    ax.add_artist(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d7533-5721-41b6-88a8-54ee7c8dc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25*4, 10*4))\n",
    "subset_num = -1\n",
    "img_size = 512\n",
    "ax.scatter(coordinates[:subset_num, 0], coordinates[:subset_num, 1])  # Plotting the points (optional)\n",
    "\n",
    "# Plot each image at its coordinates\n",
    "for coord, image, border_label in tqdm(zip(coordinates[:subset_num, :], list_all_instances_images[:subset_num], all_border_label[:subset_num])):\n",
    "    plot_image(coord[0], coord[1], image.resize((img_size, img_size)), ax, border_label)\n",
    "    \n",
    "fig.patch.set_facecolor('black')\n",
    "ax.axis('off')\n",
    "plt.suptitle(f\"Analysis for images of visual category: {id2label[class_idx]}\", fontsize=16, fontweight='bold', color='white')\n",
    "fig.savefig(f\"class_{class_idx}_{id2label[class_idx]}.pdf\", bbox_inches='tight', pad_inches=0, facecolor=fig.get_facecolor())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc31ad25-1ffd-4ec4-af87-5f72fccb5624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
