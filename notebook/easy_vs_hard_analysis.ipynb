{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493e001-39ba-45e3-aeb1-2ed1db7682de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import json\n",
    "# For generating PDFs\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../')\n",
    "from dataset.hf_imagenet_dataset import HFImageNet\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f394e-7379-4af5-8e43-71fd7e6db2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dir = '../weights_CL_imagenet/with_inst_params_lr_0.8_wd_1e-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3a1d6-3e59-4b59-a77a-a828cf4e3722",
   "metadata": {},
   "source": [
    "## Load  instance-level parameters from all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6fcfc-3b3f-4b5e-bbd6-0dfde2186901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the loaded checkpoints\n",
    "epoch_instance_level_temp = [] # (nr_epochs, nr_instances)\n",
    "total_epochs = 120\n",
    "\n",
    "# Loop through the checkpoint files and load each one\n",
    "for epoch in tqdm(range(total_epochs), desc='Loading instance level parameters from all checkpoints'):\n",
    "    checkpoint_path = f'{analysis_dir}/epoch_{epoch}.pth.tar'\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        epoch_instance_level_temp.append(checkpoint['inst_parameters'])\n",
    "    else:\n",
    "        print(f\"Checkpoint file {checkpoint_path} not found.\")\n",
    "\n",
    "# Convert the entries to temperature and vertically stack across epochs\n",
    "epoch_instance_level_temp = np.exp(np.vstack(epoch_instance_level_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4e03f-1fc0-416a-8e8a-351a346ccb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_convergence = epoch_instance_level_temp[-1,:]\n",
    "print(f'Mean temperature at convergence: {np.mean(temp_convergence) :0.1f}')\n",
    "print(f'Max temperature at convergence: {np.max(temp_convergence) :0.1f}')\n",
    "print(f'Min temperature at convergence: {np.min(temp_convergence) :0.1f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd87dc-f0a2-4f97-be6e-33a62186526d",
   "metadata": {},
   "source": [
    "## Load labels for entire dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b995440-af69-4268-b4f3-d47bc27b343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_dataset = HFImageNet(split='train', transform=None)\n",
    "label_instances = np.array(imagenet_dataset.dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f12c6c-8f24-48e8-9090-26ba52bf15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of label-ids to name of classes\n",
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"imagenet-1k-id2label.json\"\n",
    "id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n",
    "id2label = {int(k):v for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc08506-2332-42b7-968c-2ca339f92944",
   "metadata": {},
   "source": [
    "## Analyze samples of particular class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7a696-199a-4af0-a3dd-65e4a78c2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx = 107\n",
    "\n",
    "# Find instances of this class\n",
    "instance_class_idx = np.where(label_instances == class_idx)[0]\n",
    "\n",
    "# Temperature \n",
    "instance_class_temp_convergence = temp_convergence[instance_class_idx]\n",
    "\n",
    "# Sort the data points based on temperature (low to high)\n",
    "sort_idx = np.argsort(instance_class_temp_convergence)\n",
    "\n",
    "instance_class_idx = instance_class_idx[sort_idx]\n",
    "instance_class_temp_convergence = instance_class_temp_convergence[sort_idx]\n",
    "\n",
    "assert np.all(temp_convergence[instance_class_idx] == instance_class_temp_convergence), 'Both results should match'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaee4e0-aed1-494d-88d7-2de0b6fc6727",
   "metadata": {},
   "source": [
    "## Capture the top-k easy and hard images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc0faf-6ff2-4c68-a331-55ddc4a6aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_images, hard_images = [], []\n",
    "topk = 10\n",
    "\n",
    "for print_idx, (instance_idx, instance_temp) in enumerate(zip(instance_class_idx, instance_class_temp_convergence)):\n",
    "\n",
    "    # Print the top-k hardest and easiest samples of the class\n",
    "    if (print_idx < topk):\n",
    "        easy_images.append((imagenet_dataset.dataset[int(instance_idx)]['image'], instance_temp))\n",
    "\n",
    "    if (print_idx >= len(instance_class_idx) - topk):\n",
    "        hard_images.append((imagenet_dataset.dataset[int(instance_idx)]['image'], instance_temp))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddafd297-9b45-4c4c-aa0b-36c7bd00828d",
   "metadata": {},
   "source": [
    "## Plot these images in a PDF and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9cab4-effd-4db0-a96c-447d337c45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, row_label, start_subplot, ax_line=False):\n",
    "    for idx, (img, temp) in enumerate(images):\n",
    "        idx = idx + 1 # subplot starts the idx from 1\n",
    "        ax = plt.subplot(2, len(images)+1, idx + 1 + start_subplot)\n",
    "        #img = img.resize((256, 256), Image.Resampling.LANCZOS)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"{temp:.2f}\", fontsize=16, color='white')\n",
    "        ax.axis('off')  # Hide axis\n",
    "            \n",
    "    plt.figtext(0.01, 0.75 if start_subplot == 0 else 0.25, row_label, va='center', ha='left', fontsize=12, weight='bold', color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c508ea-a07e-4275-b0bc-96eeecf4f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming easy_images and hard_images are populated\n",
    "plt.figure(figsize=(20, 8), facecolor='black')  # Adjust the figure size as needed\n",
    "plot_images(easy_images, \"Easy Images\\n(score < 1.0)\", 0, ax_line=True)\n",
    "plot_images(hard_images, \"Hard Images\\n(score > 1.0)\", len(easy_images)+1)\n",
    "plt.suptitle(f\"Analysis for images of visual category: {id2label[class_idx]}\", fontsize=16, fontweight='bold', color='white')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"class_{class_idx}_{id2label[class_idx]}.pdf\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a345736-bb76-43cd-9b5c-41f7b8508ec4",
   "metadata": {},
   "source": [
    "## Automated loop over all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ac350-6d73-43b9-bdc8-567086622e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will do this analysis for 100 classes sampled uniformly from the dataset.\n",
    "topk = 5\n",
    "\n",
    "for class_idx in range(0, 1000,10):\n",
    "    \n",
    "    # Find instances of this class\n",
    "    instance_class_idx = np.where(label_instances == class_idx)[0]\n",
    "    \n",
    "    # Temperature \n",
    "    instance_class_temp_convergence = temp_convergence[instance_class_idx]\n",
    "    \n",
    "    # Sort the data points based on temperature (low to high)\n",
    "    sort_idx = np.argsort(instance_class_temp_convergence)\n",
    "    \n",
    "    instance_class_idx = instance_class_idx[sort_idx]\n",
    "    instance_class_temp_convergence = instance_class_temp_convergence[sort_idx]\n",
    "\n",
    "    easy_images, hard_images = [], []\n",
    "    \n",
    "    for print_idx, (instance_idx, instance_temp) in enumerate(zip(instance_class_idx, instance_class_temp_convergence)):\n",
    "    \n",
    "        # Print the top-k hardest and easiest samples of the class\n",
    "        if (print_idx < topk):\n",
    "            easy_images.append((imagenet_dataset.dataset[int(instance_idx)]['image'], instance_temp))\n",
    "    \n",
    "        if (print_idx >= len(instance_class_idx) - topk):\n",
    "            hard_images.append((imagenet_dataset.dataset[int(instance_idx)]['image'], instance_temp))        \n",
    "\n",
    "    # Assuming easy_images and hard_images are populated\n",
    "    fig= plt.figure(figsize=(20, 8), facecolor='black')  # Adjust the figure size as needed\n",
    "    plot_images(easy_images, \"Easy Images\\n(score < 1.0)\", 0, ax_line=True)\n",
    "    plot_images(hard_images, \"Hard Images\\n(score > 1.0)\", len(easy_images)+1)\n",
    "    plt.suptitle(f\"Analysis for images of visual category: {id2label[class_idx]}\", fontsize=16, fontweight='bold', color='white')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"easy_vs_hard/class_{class_idx}_{id2label[class_idx]}.pdf\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f185c-edc6-49a5-a864-159d982345b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
